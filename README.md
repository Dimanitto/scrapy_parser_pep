# Проект асинхронного парсера PEP

## Технологии:
 ![GitHub](https://img.shields.io/badge/-GitHub-464646??style=flat-square&logo=GitHub)  ![Python](https://img.shields.io/badge/-Python-464646??style=flat-square&logo=Python) * Scrapy

## Описание проекта
Проект представляет собой парсер веб-сайта [документов PEP](https://www.python.org/dev/peps/) на базе фреймворка Scrapy. В рамках проекта реализован парсер, собирающий сводные данные по PEP. Данные сохраняются в два csv-файла: в первом файле сохраняется список всех PEP (номер, название и статус), во втором - сводные данные по статусам РЕР (сколько найдено документов в каждом статусе).
___
## Как запустить проект

Клонируйте репозиторий, перейдите в папку, создайте виртуальное окружение и активируйте:
```
python3 -m venv env
```
```
. venv/bin/activate
```

Обновите менеджер пакетов (pip) и установите зависимости из файла requirements.txt:

```
(venv) python3 -m pip install --upgrade pip
```
```
(venv) pip install -r requirements.txt
```
___
## Запуск парсера:
```
scrapy crawl pep
```
___
## Результаты работы парсера:
Парсер выводит собранную информацию в два файла .csv в папку results/:
- В первом файле (`pep_{Date}.csv`) - список всех PEP: номер, название и статус.
 

- Во втором файле (`status_summary_{Date}.csv`) содержится сводка по статусам PEP — 
  сколько найдено документов в каждом статусе (статус, количество) и общее количество всех документов.


### Дополнительно
Удобно работать/тестировать в интерактивной оболочке Scrapy Shell:
◾  Через командную строку в директории scrapy_parser_pep запустите скрипт:

    scrapy shell 'URL'
Где URL —   веб-страница или путь до локального HTML-файла
После запуска Scrapy Shell в терминал будут выведены логи запроса и краткая справка по доступным объектам и командам
___
### Автор
Selivanov Dmitry